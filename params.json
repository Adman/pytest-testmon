{"name":"pytest-testmon","tagline":"taking TDD to a new level with testmon and py.test for Python","body":"\r\n[![Build Status](https://travis-ci.org/tarpas/pytest-testmon.svg?branch=master)](https://travis-ci.org/tarpas/pytest-testmon)\r\n\r\nThis is a py.test plug-in which automatically selects and re-executes only tests affected by recent changes. How is this possible in dynamic language like Python and how reliable is it? Read here: [Determining affected tests]\r\n\r\nNew versions usually have new dataformat, don’t forget to rm .testmondata after each upgrade.\r\n\r\ntestmon is approaching completeness. Unfortunatelly the classic console UI is reaching it’s usability limits even without testmon. With testmon it’s even a little more difficult to determine which tests are beeing executed, which are failing and why. Next step would be an implementation or integration of GUI. I don’t like any of the existing graphical test runners, so if you have some better new concept in mind, get in touch!\r\n\r\nUsage\r\n=====\r\n\r\n    pip install pytest-testmon\r\n\r\n    # build the dependency database and save it to .testmondata\r\n    py.test --testmon\r\n\r\n    # list of watched project files ordered by tests which reach each specific file\r\n    py.test --by-test-count\r\n\r\n    # change some of your code (with test coverage)\r\n\r\n    # only run tests affected by recent changes\r\n    py.test --testmon\r\n\r\n    # start from scratch (if needed)\r\n    rm .testmondata\r\n\r\n    # automatic re-execution on every file change with pytest-watch (https://github.com/joeyespo/pytest-watch)\r\n    pip install pytest-watch\r\n    ptw -- --testmon\r\n\r\nOther switches\r\n--------------\r\n\r\n**–project-directory=** only files in under this directory will be tracked by coveragepy. Default is rootdir, can be repeated\r\n\r\nConfiguration\r\n=============\r\n\r\nAdd testmon to the pytest.ini\r\n\r\n    [pytest]\r\n    #if you want to separate different environments running the same sources\r\n    run_variant_expression = os.environ.get('DJANGO_SETTINGS_MODULE') + ':python' + str(sys.version_info[:2])\r\n    addopts = --testmon # you can make --testmon a default if you want\r\n\r\nThoughts\r\n========\r\n\r\nIndividual test outcomes depend on many things, so let’s write a little about some of them.\r\n\r\n1.  executed python code inside the tested project (which presumably changes very frequently, little by little)\r\n2.  environment variables (e.g. DJANGO\\_SETTINGS\\_MODULE), python version (the run\\_variant\\_expression config value denotes these)\r\n3.  executed python code in all of the **libraries** (which presumably change infrequently)\r\n4.  **data files** (txt, xml, other project assets)\r\n5.  external services (reached through network)\r\n\r\n**testmon** so far deals with incrementally running tests when faced with the 1. and 2. category of changes.\r\n\r\nLater versions can implement some detection of other categories\r\n\r\n**libraries**: we could compare pip freeze between runs, but it’s slow\r\n\r\n**data files**: Probably the best bet here is a configuration where the developer would specify which files does a\r\n\r\n  [Determining affected tests]: https://github.com/tarpas/pytest-testmon/wiki/Determining-affected-tests","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}